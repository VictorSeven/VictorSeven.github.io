---
layout: post
title:  "Reseña Crítica: Emergence and Convergence, de M. Bunge"
date:   2020-09-30 13:00:00 -0500

tags: libro bunge complejos sistemas

summary: "Una visión personal del libro de Bunge sobre pensamiento sistémico"

tipo: divul
head_image: "/images/2020/bunge.png"
---

Con motivo de la escritura de mi tesis doctoral (que ya ha comenzado, a un ritmo más lento del que me gustaría) he estado leyendo el libro de Mario Bunge, _Emergence and Convergence: Qualitative Novelty and the Unity of Knowledge_, para ponerme un poco al día de la filosofía que acompaña al pensamiento sistémico en ciencia. Como mi tesis es básicamente física estadística aplicada a biología, creo que leer y entender al menos un libro de filosofía sobre el tema me viene bien.

Os comparto aquí mis pensamientos sobre el libro. Para todo aquel que no quiera leer la crítica detallada a mis apuntes, diré básicamente que el libro no me ha gustado demasiado. En forma corta,

- Bien: Bunge se explica de forma bastante clara, tiene abundantes ejemplos, y toca todos los temas desde física, química a ciencias sociales. Los mensajes importantes del libro están muy claros. El libro está bien estructurado, el propósito de cada capítulo es claro y realmente la lectura aporta bastantes cosas. En el mensaje principal de Bunge puedo decir que estoy completamente de acuerdo: la ciencia ha de evaluarse de forma sistémica, evitando el reduccionismo extremo por un lado y el holismo sin sentido por el otro. Es importante entender bien los componentes de cada sistema, cómo se estructuran y cómo funcionan, pero luego hay que hacer el camino inverso de entender el sistema como un todo, para tener un entendimiento adecuado. Para ello, adicionalmente, puede hacer falta la colaboración estrecha de varias disciplinas. Otras metodologías pueden salir mal... y han salido mal, como Bunge describe en el libro. El pensamiento sistémico debe ser la síntesis de la filosofía subyacente a la metodología científica (hasta que aparezca algo mejor, pero de momento es un excelente candidato)

- Mal: el libro se repite bastante. Le da varias veces seguidas vueltas a las mismas ideas sin aportar gran cosa. Es cierto que aunque en entre capítulos se avanza, cada capítulo a veces es reiterativo. Además, he detectado bastantes problemas serios en algunos de los ejemplos (definiciones incorrectas, reduccionismos involuntarios, o sencillamente críticas que no comparto en absoluto). Creo que esto es peligroso para el lector despistado y la verdad es que es lo que más me preocupa en el libro: puedo detectar con cierta facilidad deslices en la parte de física, informática o biología, pero no en psicología, sociología, lenguaje o alguna lógica formal (sin una revisión profunda). Eso produce que al final no me fíe demasiado de algunos de sus datos o ejemplos.

En resumen, las ideas principales bien, pero a veces el libro es repetitivo y no me fío demasiado dle contenido. Para que vosotros tengáis vuestra propia opinión, os voy a dejar abajo una lista de problemas que he ido encontrando.  Empezaré con lo que estoy bastante seguro, son errores o al menos deberían haber pasado por una corrección para explicarse mejor, para luego dar algunos puntos discutibles con los que no estoy de acuerdo. 

## Algunas cosas en las que no confío

**Capítulo 1, Sección 4.** Bunge presenta un par de postulados sobre la evolución de los sistemas, en lenguaje natural. Básicamente: que toda evolución implica la emergencia de nuevas propiedades, y que la habilidad de cambiar es común a todas las cosas. Inmediatamente después, presenta a partir de estos postulados un teorema que afirma que algunos cambios son graduales, y otros 'discontinuos'. El teorema aparece sin ninguna clase de demostración o razonamiento a partir de los postulados, ni cita donde consultarlo. No es algo que me parezca muy riguroso de su parte.

**Capítulo 3, Sección 3**:  aquí se habla de la diferencia entre la mecánica Newtoniana y la formulación de la mecánica de Lagrange y Hamilton. Se dice que la mecánica Newtoniana no es tan adecuada porque trabaja con las fuerzas que actúan sobre cada cuerpo de forma individual, mientras que la Hamiltoniana funciona mejor, al trabajar el sistema como un todo. En realidad, esto no es cierto. Tómese el sistema solar (justo el ejemplo que emplea arriba Bunge). Escribir las ecuaciones que describen en movimiento de todos los planetas, con el método de Newton es directo: solo hay que sumar fuerza de cada planeta, y esto corresponde al producto de la masa por la aceleración. Si quiero hacerlo mediante mecánica Lagrangiana, debo sumar toda la energía cinética y potencial de todos los planetas, y después calcular una serie de derivadas parciales. Y es más, si quiero la formulación Hamiltoniana, aún tengo que hacer una transformada de Legendre y hacer otra serie de derivadas parciales. ¿Entonces, dónde está la ventaja?

La respuesta está en cualquier curso de mecánica analítica. La formulación Newtoniana sufre cuando los cuerpos tienen _ligaduras_. Por ejemplo, un cuerpo que se encuentra en la superficie de una mesa bajo la acción de la gravedad. El cuerpo no puede atravesar la mesa, pero está sujeto a la gravedad. Si solo la gravedad actúa, obtendré una ecuación que me dice que el cuerpo acelera hacia abajo. Para evitarlo, tengo que 'inventar' la fuerza normal, una fuerza que ejerce la mesa sobre el objeto y es exactamente igual que la gravedad, de signo contrario.  Si tuviera una bola ensartada en un alambre en espiral, y la bola va deslizando por la espiral debido a la gravedad, tengo que añadir otra fuerza que impida que la bola se salga de la trayectoria de la espiral. Escribir esta fuerza _ad hoc_, matemáticamente,  es muy difícil. Sin embargo, resulta que en estos casos escribir la energía total del cuerpo que cae es mucho más sencillo, lo que hace la formulación lagrangiana más adecuada. En cuanto a la Hamiltoniana, se utiliza porque es una función especial que tiene una serie de propiedades matemáticas muy convenientes. El hecho de que provenga del extremal de una acción le confiere una serie de propiedades útiles, especialmente cuando hablamos de simetrías. Pero en ambas formulaciones al final lo que estamos haciendo es sumar bien todas las fuerzas, bien todas las energías, y luego aplicarlas a cada cuerpo. En el caso de ligaduras, la formulación Lagrangiana es más rápida. En partículas libres, Newton sale de forma mucho más natural. Si quiero hacer virguerías formales, Hamilton-Jacobi tiene una serie de herramientas muy útiles para reducir el sistema a unas coordenas más adecuadas. 

**Capítulo 9, Sección 1.** _[La interpretación de Copenhague de la mecánica cuántica] postula que cada proceso microfísico se produce por un experimentador usando un dispositivo de medida. [Esto es holismo porque] el micro dependería de lo macro, y no habría micro-procesos, excepto en el laboratorio_. Si no me equivoco, esta es la visión personal de Bohr y Heisenberg, que ha sido revisada más tarde. En cualquier caso, claro que hay procesos microfísicos, y ellos mismos eran conscientes de ello: en ausencia de medida, son los descritos por la ecuación de Schödinger. Sin embargo, la crítica sí sería válida al axioma de la medida que defendía especialmente Heisenberg, quien pensaba que la incertidumbre cuántica venía de perturbar el propio sistema. Sin embargo, hoy sabemos que no es así, sino que la incertidumbre viene por el carácter ondulatorio del campo.

**Capítulo 9, Sección 6.** Afirma que los ecosistemas más diversos no son más estables. Esto es falso y precisamente es una de las preguntas más interesantes de ecología teórica actual, cómo obtienen los ecosistemas su estabilidad. Referencias:

 **Capítulo 9, Sección 6.** Establece que la psicología no es completamente reducible a la biología (algo en lo que estoy de acuerdo, por supuesto; pero no por este motivo:) porque la psicología emplea conceptos de su propia cosecha, como emoción, consciencia o personalidad, que no están presentes en la biología. Que yo sepa, la temperatura tampoco está presente en la física atómica, pero es perfectamente posible explicarla a través de las propiedades del conjunto de átomos.

 **Capítulo 9, Sección 7.** Critica a la teoría de la _kin selection_. Esta establece que los individuos de una especie tienen a ser altruistas y/o reproducirse con aquellos que están más 'cercanos'. En muchas especies, este 'cercanos' se refiere literalmente a distancia genética. Bunge critica que a nivel social, esto no ocurre con las personas, que pueden ser más altruistas con personas que no están emparentadas. Por otro lado, para reforzar esta tesis emplea argumentos del tipo 'una persona puede comportarse de forma altruista con algo que no tenga ninguna relación biológica, como una mascota'.  En contra de estos dos argumentos, hay que recordar, primero, que _kin selection_ es una teoría que explica el comportamiento a nivel de la población, y no de los elementos individuales -cada uno de los individuos no estamos obligados a cumplirla, y por ende a nivel individual hay excepciones. Segundo, en humanos el altruismo se puede dar entre gente con poco parentesco genético, pero el cuidado de los niños por sus padres sigue siendo mayoritario. En cualquier caso, los sociólogos que se adhieren a esta teoría argumentan que debe aplicarse a grupos sociales, y no por genes. Así, una persona 'cercana' a mí es una persona con la que socialmente se comparten cosas. De otra forma estaríamos reduciendo todo el emparejamiento, actividad muy influenciada por etiquetas sociales y culturales, a los genes, mientras que el libro precisamente advierte de 'cuidado con el reduccionismo genético'. Aquí me da que Bunge cae en su propio reduccionismo, poniendo toda la _kin selection_ a nivel genético, lo cual diría que no es una idea compartida actualmente.  Y en los casos en los que es genético hay más mecanismos de selección natural -esta selección en particular es muy importante en algunas especies, y no tanto en otras. 

**Capítulo 9, Sección 8.** Bunge se refiere a la hipótesis de _caos molecular_ en teoría cinética. Según él, esta hipótesis implica una distribución inicial de posiciones y velocidades aleatoria. Esto no es cierto. La hipótesis de caos molecular implica que las velocidades y las posiciones de las partículas no están correlacionados en ningún momento. Si la hipótesis no fuera cierta, incluso la distribución inicial de posiciones y velocidades desarrollaría correlaciones a lo largo del tiempo. Es cierto que si hay cierto orden en las condiciones iniciales esto puede llevar posteriormente a correlaciones, pero diría que el recíproco no es cierto.

**Capítulo 10, Sección 1.** Afirma que la física más fundamental, o la física cuántica, no puede saber nada de la biología, entre otras cosas, porque 'nadie sabe cómo escribir una ecuación de Schödinger para una bacteria, ni siquiera la más sencilla'. Es cierto que hay que tener en cuenta las propiedades emergentes, y la biología no es reducible sin más a la física, pero no es por este motivo. Como el propio Bunge afirma, las propiedades emergentes son ontológicas, porque el sistema _tiene_ esas propiedades, y no epistemológicas -no dependen de que entendamos cómo tiene lugar la emergencia. El hecho de que nadie _sepa_ escribir una ecuación de Schödinger para una bacteria no implica que no se pueda: el formalismo de sistemas cuánticos abiertos, en teoría, lo permite; y analizando esas ecuaciones podríamos obtener algunas de las propiedades emergentes a partir de los componentes de la bacteria. Es una tarea titánica, pero sí es plausible y daría mucha información sobre los procesos que tienen lugar en el organismo. Una cosa es que la bacteria no sea reducible a la ecuación de Schrödinger de unas pocas moléculas, y otra que utilizando el conocimiento de física que tenemos, hagamos un modelo que incluya a las moléculas, las interacciones entre ellas, y los flujos de energía de la bacteria para entenderla. Y ahora sí, estaríamos entendiendo las propiedades de la célula _desde abajo_. El proceso que el mismo Bunge llama _convergencia_. 

**Capítulo 12, Sección 1.** Afirma que defenderá el argumento de que el cerebro, a pesar de estar compuesta por módulos fisiológicamente, tiene una estructura _integral_ más que modular. Esto no es cierto, ya que funcionalmente la actividad está también muy centrada en módulos, tanto en reposo como cuando se realizan tareas, como en resonancias y electroencefalogramas.

**Capítulo 12, Sección 2.**  Aquí realiza una crítica al 'coneccionismo', entendendido como la filosofía de que la topología de las conexiones es lo más importante en el sistema. Aunque estoy de acuerdo con Bunge en que el coneccionismo total no es bueno, hay que recordar que la estructura del sistema es muy importante, y, en algunos casos, la responsable última de algunos fenómenos. Después, Bunge lanza varios ataques a los modelos de simulación en neurociencia, a los que acusa de seguir esta (mala) filosofía. Ya que esta es mi área, me dedicaré a rebatir los comentarios que me parecen más desafortunados. Por ejemplo, afirma que las 'simulaciones por ordenador no prueban nada' y que 'el modelado no puede sustituir nunca al trabajo en el laboratorio'. Estoy totalmente en desacuerdo con la primera y parcialmente de acuerdo con la segunda.   Para defender esta hipótesis, Bunge afirma que el problema radica en que las neuronas simuladas son modelos ultrasimplificados de las verdaderas, y que no modelan todo el tejido del cerebro: el baño hormonal está ausente, las sinpasis a menudo se sustituyen por pesos y funciones de input, y que estos no son como los 'modelos matemáticos tradicionales'. Además, no hay ninguna actividad espontánea, que sea independiente de algunos estímulos, y por eso fallan el criterio de inteligencia de Claparede: no son capaces de resolver nuevos problemas.

Vamos paso a paso: 

1. No poder simular el cerebro entero no es un problema. Como el mismo Bunge afirma en el libro, las conexiones y relaciones entre los sistemas en el universo no son exactamente iguales. Yo no estoy conectado con Venus. Por tanto, por mucho que nos guste la visión sistémica, la interacción entre Venus y yo es tan irrisoria que puede ser ignorada, y eso permite a la ciencia parcelar el conocimiento. Con el cerebro ocurre lo mismo: debido a su estructura modular, las neuronas dentro de cierta región interaccionan mucho más entre ellas que con otras regiones. Es cierto que puede haber cierto flujo de corriente entrante o saliente de la región, debido a muchos motivos, pero es posible simular regiones concretas sin que suponga un problema. Obviamente esto no nos da una imagen de cómo funciona el cerebro al completo, pero sí de cómo funciona X región. Por otro lado, siempre es posible obtener y comparar informaciones con neuronas in-vitro. Como digo, esto no desvela cada detalle del cerebro, y entra un poco más dentro de un plan reduccionista, de entender bien cómo funciona cada componente del cerebro por separado. Pero por supuesto, la idea es ir juntando las piezas del puzle para ir subiendo poco a poco, explicando cada vez más y mejor (o coger el conocimiento específico de esa región para hacer aplicaciones médicas). En los últimos 10 años tenemos ya muchos modelos computacionales del 'cerebro completo' donde las unidades no son neuronas individuales, son regiones enteras, cuyos modelos se han construido a partir de la información de otras simulaciones, trabajo experimental, y observaciones fisiológicas. ¡Y en el 'Human Brain Project' ya sí atacan toda la estructura, incluyendo todos los detalles posibles!
2. Los modelos computacionales no son distintos de los modelos matemáticos tradicionales. El formalismo es igual que el de otras muchas ramas de la matemática aplicada (como los modelos de compartimentos en epidemiología o los modelos poblacionales). Todos esos modelos son ecuaciones diferenciales, que se han obtenido a base de estudiar con mucho cuidado, experimentalmente, la física de las neuronas. Cómo hacen para cambiar su potencial eléctrico, cómo envían impulsos, etc. Muchos de los parámetros usados en los modelos más realistas no son 'artefactos'. Bunge denuncia que el parámetro de acoplamiento utilizado entre las sinapsis de las neuronas neuronas son pesos arbitrarios, pero eso solo así en estudios lo que se llama 'toy models', o modelos de juguete. En los estudios más realistas se suelen utilizar los datos de conductancias reales entre las sinapsis, así como funciones de interaccionar obtenidas de estudiar el comportamiento eléctrico de la neurona. ¿Qué pasa con los modelos más simples? Pues obviamente cuando uno utiliza un modelo esquemático o de juguete tiene que ser consciente de cuáles son exactamente sus limitaciones. Si el modelo de juguete da algo contrario a lo que dice un modelo de la neurona más realista, se desecha -o mejor aún, se comparan a fondo ambos modelos para entender qué produce exactamente la diferencia-. Si el modelo de juguete funciona y da predicciones (cualitativas) de calidad, dada su simplicidad, podemos utilizarlo aprender cuáles son las propiedades que están produciendo el efecto emergente. Ontológicamente creo que todo el mundo entiende que un modelo de apagado/encendido con un par de probabilidades no es una neurona, ni se le puede parecer, y que las neuronas reales tienen muchos más detalles que habrá que ir entendiendo. Pero tal vez sea lo suficientemente sencillo como para que, epistemológicamente, me sirva para aprender algunas de las propiedades que presenta un sistema de neuronas de verdad.
3. Según Bunge en estos modelos no hay ninguna actividad espontánea o no viene ningún input de fuera, a no ser que sean estímulos puestos por investigadores conductistas... No sé qué modelos ha mirado Bunge para escribir el libro. Sinceramente me sorprende que siendo el libro de 2003, los modelos que mencione sean modelos que imitan el comportamiento de la memoria, hagan cosas sencillas de poda sináptica, y cuenten (según el propio Bunge) con unas 200 neuronas poco realistas. Cuando en 1980 ya teníamos modelos (sin ir más lejos, Van Vreesvik y Sompolinsky, en Science) con decenas de miles de neuronas con un nivel de realismo biológico mucho más alto, con modelos en los que se intenta reproducir la actividad espontánea del cerebro,  que incluyen inputs externos o actividad espontánea. Incluso los modelos mesoscópicos de Wilson y Cowan de 1950 ya hablan (y resuelven parte de) todos estos problemas. Parece que Bunge se ha estado fijando en los perceptrones cutres de los 80 que se utilizaban con el sueño de reconocer muchos patrones y vender que las máquinas podían ser muy inteligentes -con relativamente poco éxito en aquella época- en vez de mirar los modelos académicos y maduros que había por aquel entonces. Y de todas formas, aunque hubiera sido así, sus críticas han caído finalmente en el vacío: la IA moderna, por ejemplo GPT-3, puede resolver muchos más problemas para los que -en teoría- no se le ha entrenado. Hacer operaciones matemáticas, traducir haikus, escribir un blog o código en Python, cuando es una IA utilizada en principio para generar lenguaje natural. 

**Capítulo 12, Sección 6.** En este caso se hacen críticas a la inteligencia artificial. Para empezar, afirma que el objetivo de la inteligencia artificial es simular el comportamiento animal. Creo que si le preguntar a la mayor parte de mis amigos especialistas en IA qué objetivo persiguen para IA esta no sería ninguna de las respuestas. Diría en general el objetivo principal de la inteligencia artifical [(y la Wikipedia está de acuerdo conmigo)](https://es.wikipedia.org/wiki/Inteligencia_artificial) es conseguir que los ordenadores sean capaces de abstraer información en un conjunto de datos, y los utilicen para realizar con éxito una tarea de interés para el investigador: clasificar fotos, predecir series temporales, crear arte, o incluso interaccionar con una persona. El objetivo de una IA que clasifica imágenes de galaxias no es parecerse a ningún a nivel, es clasificar imágenes que para nosotros son complicadas con una facilidad sobre-humana. El objetivo de un *chatbot* no es tener sentimientos o memoria, es ser capaz de entender lo que le está preguntando el cliente, en lenguaje natural, y actuar en consecuencia. Una AI que genera melodías musicales al estilo de Bach no necesita parecerse a ningún animal: solamente generar melodías similares a las que haría Bach. El concepto de IA al que se refiere Bunge es muchísimo más reducido, y se trata de simular una persona o animal de forma realista, lo cual, como defiende él, requiere primero entender bien cómo funciona una persona o animal, a nivel psicológico. Por otro lado, Bunge dice literalmente que *motivaciones, sentimientos, emociones, espontenidad y otros similares no son ni algorítmicos ni útiles en una máquina.* A esto, dos criicas: una flojita, y tal vez discutible, es el hecho de que no conozcamos un algoritmo o un sistema simple cuyas propiedades emergentes sean emociones no quiere decir que no existan. Que no sean útiles es falso. Un ordenador de atención al usuario automatizado que fuera capaz de entender las emociones de la persona con la interactúa y proceder en consecuencia es claramente superior, a nivel tecnológico, que uno que no entiende dichas emociones; un ordenador capaz de crear nuevas respuestas espontáneas a las preguntas que se le hacen, o que ayude al compositor a crear arte, tales como dibujos, melodías o ideas, es claramente útil en el mundo actual. Al fin y al cabo, ¿qué músico no quiere un robot que le genere acompañamientos automáticos para practicar el arte de la improvisación?

### Pequeña discusión: el concepto de aleatoriedad

 Bunge se queja en varias ocaciones de que al estudiar ciertos sistemas, se le asignen probabilidades a ciertos eventos que no son aleatorios, ontológicamente hablando, y echa buena parte de la culpa a la interpretación Bayesiana, acusándola de subjetivista. Por un lado, estoy de acuerdo que a veces se considera aleatorio a un proceso que está generado por individuos deterministas, y esto puede resultar chocante. Sin embargo, precisamente al comportamiento emergente de los sistemas, sus propiedades pueden llegar a ser tan indistiguibles de la aleatoriedad más pura, que me ha llevado a pensar que verdaderamente el propio concepto aleatoriedad es emergente. La verdad es que quería explicarlo aquí, pero el post se va a hacer muy largo y necesitaría incluir cuentas, así que dejaré esta entrada en un *cliffhanger*. ¿Queréis saber por qué la aleatoriedad es emergente? ¡Pues no os perdáis la próxima entrada!

Con lo cual, sin más, dejo aquí mi crítica al libro de Bunge. Recapitulo mis conclusiones: buen libro, pero a veces un poco repetitivo de leer, y en ocasiones no me fío demasiado de las cosas que dice, porque he podido detectar alguna imprecisión que os he contado arriba. Por otro lado, creo que puede ser un buen libro como introducción al pensamiento sistémico, en especial los primeros capitulos, y que puede dar un soplo de aire fresco a la gente muy metida en reduccionismo extremo.

¡Nos veremos pronto: para hablar de aleatoriedad emergente, como os he prometido!